<module title="Programmation parallèle"
with-contents="true"
ocaml-session="para"
draft="true"
>
<contents>
<section id="introduction" title="Introduction">
<p>
OCaml permet différentes sortes de parallèlisme/concurrence:
</p>
<ul>
<li>par l'utilisation de threads (ou processus légers),</li>
<li>via des processus lourds et de la communication inter-processus,</li>
<li>ou en utilisant des bibliothèques permettant d'utiliser des monades
  pour ne pas bloquer le programme sur des opérations potentiellement bloquantes
  (lectures/écritures de fichiers, entrées/sorties réseau, attente d'événements
   d'interface graphiques, ...).</li>
</ul>
<p>
Parallèlisme et
concurrence sont deux choses différentes<note>On pourra lire à ce sujet
<ext-a href="http://existentialtype.wordpress.com/2014/04/09/parallelism-and-concurrency-revisited/">ce billet de Robert Harper</ext-a>.</note> Il est tout à fait possible d'avoir
des calculs en parallèle sans concurrence, de même que les problèmes de concurrence
peuvent être traités avec un pseudo-parallèlisme, comme c'est le cas avec
la bibliothèque <ext-a href="http://ocsigen.org/lwt/">Lwt</ext-a>.
</p>
<p>
Le problème d'OCaml vis-à-vis du parallèlisme/de la concurrence est que le
glaneur de cellules (<em>garbage collector</em>, ou GC) n'est pas concurrent
et que lorsqu'il entre en action, il bloque tout le processus en cours,
y compris les éventuels threads (processus légers).
Des travaux de recherche sont en cours pour fournir un GC concurrent à OCaml.
</p>
<p>
Ceci étant dit, nous allons maintenant explorer les différentes solutions
pour effectuer des calculs en parallèle et/ou en concurrence, selon les
trois approches listées plus haut. Chaque fois, nous listerons les
bibliothèques utilisables mais nous contenterons d'exemples sur seulement
certaines d'entre elles.
</p>
</section>

<section id="threads" title="Processus légers">
<p>Le but de cette introduction n'est pas de présenter ce que
sont les processus légers. On pourra se référer par exemple
à <cite href="sysocaml"/>.
</p>
<subsection id="ocamlthreads" title="Threads en OCaml">
<p>
Les processus légers sont gérés à l'aide du module
<moduledoc name="Thread"/>. Ils sont disponibles sur plateformes
Posix 1003.1c et Win32.
</p>
<p>
La compilation de modules utilisant les threads nécessite de passer
l'option <code>-thread</code> aux compilateurs <code>ocamlc</code>
et <code>ocamlopt</code>, ainsi que de lier avec la bibliothèque
<code>threads.cma</code> (pour le code-octet) ou <code>threads.cmxa</code>
(pour le code natif). Comme cette bibliothèque nécessite également
la biblitothèque Unix, on liera le programme final avec aussi
<code>unix.cma</code> ou <code>unix.cmxa</code>.
</p>
<warning id="nativethreads" title="Threads natifs ou émulés">
<p>
Si le système pour lequel est compilé le code utilisant des threads
ne supporte pas les threads, il faut remplacer l'option <code>-thread</code>
par l'option <code>-vmthread</code>. Tous les threads sont alors exécutés
dans le même processus. Cette option n'est cependant disponible que pour
la compilation en code-octet.
</p>
</warning>
<exercice id="thr1" title="Affichages en parallèle">
<p>Un exercice classique d'utilisation des threads est l'affichage
de messages en parallèle.
Ecrire un programme créant 3 threads, qui affiche chacun 10 lignes
de la forme "Je suis le thread X et c'est mon affichage N.".
</p>
<p>Attention à bien attendre la fin des threads, sinon le programme
principal termine, terminant également les threads avant qu'ils aient
fini leur travail.
</p>
<p>
Si le code se trouve dans un fichier <code>thread_print.ml</code>,
on pourra le compiler par la commande suivante:
</p>
<sh>$ ocamlopt -o thread_print -thread unix.cmxa threads.cmxa thread_print.ml</sh>
<solution>
<ocaml id ="thrprint" defer_="1"><include raw="true" file="&lt;stog-dir/&gt;/codes/thread_print.ml"/></ocaml>
</solution>
</exercice>
<p>
Différents modules sont également disponibles pour la communication
entre threads:
</p>
<ul>
<li><moduledoc name="Mutex"/> pour la gestion des section critiques (exclusion mutuelle),</li>
<li><moduledoc name="Condition"/> pour attendre ou signaler une condition,</li>
<li><moduledoc name="Event"/> pour une programmation basée sur des événements envoyés
sur des canaux.</li>
</ul>
</subsection>

<subsection id="async" title="Async">
<p>
La bibliothèque <ext-a href="http://janestreet.github.io/">Async</ext-a> facilite
l'utilisation de threads ...
</p>
Comment Async fonctionne: http://alan.petitepomme.net/cwn/2014.06.10.html#2

</subsection>
</section>

<section id="process" title="Processus lourds">
<p>
La majorité des bibliothèques permettant la parallélisation utilise des processus
lourds, exécutés soit en local, soit à distance en se connectant à d'autres machines
pour y créer des processus.
</p>
<p>
OCaml, via le module <moduledoc name="Unix"/>, offre l'accès aux appels systèmes
permettant la création de processus (<em>fork</em>), la communication via des
<em>pipes</em> et des <em>sockets</em>, etc.
</p>
<p>
Il existe cependant plusieurs bibliothèques offrant des abstractions pour faciliter
ces modèles de calcul parallèle et/ou distribué:
</p>
<ul>
<li><ext-a href="https://github.com/janestreet/async_parallel">Async-parallel</ext-a>,
une extension d'Async pour l'exécution de code en parallèle sur des machines distantes,</li>
<li><ext-a href="http://rdicosmo.github.io/parmap/">Parmap</ext-a> permet l'exécution
de fonctions sur plusieurs <em>cœurs</em>,</li>
<li><ext-a href="http://projects.camlcity.org/projects/ocamlnet.html">OCamlnet</ext-a>,
une bibliothèque offrant beaucoup de fonctions pour développer des applications utilisant
le réseau, contient plusieurs modules pour la programmation parallèle ainsi que
pour le partage de structures de données et le passage de messages,</li>
<li><ext-a href="http://forge.ocamlcore.org/projects/ocamlmpi/">OCamlMPI</ext-a> est
une interface avec la bibliothèque de passage de messages MPI,</li>
<li><ext-a href="http://functory.lri.fr/">Functory</ext-a> est une bibliothèque permettant
d'effectuer des calculs séquentiellement ou en parallèle, soit sur la même machine,
soit sur des machines distantes.</li>
</ul>
<subsection id="functory" title="Functory">
<p>
<ext-a href="http://functory.lri.fr/">Functory</ext-a> présente l'avantage d'avoir
une interface commune pour les différentes façons de distribuer les calculs
à effectuer, ainsi qu'un module pour l'exécution séquentielle, facilitant le
débogage et/ou l'obtention d'un comportement de référence.
</p>
<p>
En guise d'exemple, nous allons écrire un programme qui compte le nombre de caractères
de chaque fichier du répertoire courant, la première fois de façon séquentielle,
la seconde en utilisant plusieurs processus locaux.
</p>
<p>
Nous définissons tout d'abord une fonction retournant la liste des fichiers
d'un répertoire en paramètre:
</p>
<oc-eval>let ls_dir dir =
  try
    let handle = Unix.opendir dir in
    let rec read acc =
       match
         try Some(Unix.readdir handle)
         with End_of_file -> None
       with
         None -> acc
       | Some file ->
           let f = Filename.concat dir file in
           match (Unix.stat f).Unix.st_kind with
             Unix.S_REG -> read (file :: acc)
           | _ -> read acc
     in
     read []
  with
    Unix.Unix_error (e, s1, s2) ->
      failwith (Printf.sprintf "%s: %s %s" (Unix.error_message e) s1 s2)
;;
</oc-eval>
<p>Nous définissons également une fonction retournant la taille du fichier
en paramètre:
</p>
<oc-eval>let file_size file =
  try (Unix.stat file).Unix.st_size
  with Unix.Unix_error (e, s1, s2) ->
    failwith (Printf.sprintf "%s: %s %s" (Unix.error_message e) s1 s2)
;;
</oc-eval>
<p>
Enfin, nous utilisons le module de la bibliothèque Functory effectuer
les opérations séquentiellement, pour appliquer la fonction <ml>file_size</ml>
sur chaque fichier du répertoire courant:
</p>
<oc-eval>Functory.Sequential.map (fun f -> (f, file_size f)) (ls_dir ".") ;;
</oc-eval>
<p>
Si nous souhaitons effectuer le même traitement, mais en utilisant plusieurs
cœurs de notre machine, il nous suffit d'indiquer le nombre maximum de cœurs
à utiliser et d'appeler la fonction <ml>map</ml> du module <ml>Functory.Cores</ml>:
</p>
<oc-eval>Functory.Cores.set_number_of_cores 4;;
Functory.Cores.map (fun f -> (f, file_size f)) (ls_dir ".") ;;
</oc-eval>
<p>On peut ainsi, en changeant par exemple un <ml>open</ml>, utiliser
soit le <ml>map</ml> séquentiel, soit le <ml>map</ml> parallèle local.
</p>
<p>
De la même façon, il est possible de lancer les calculs sur des machines
distantes, préalablement déclarées, à l'aide du module <ml>Functory.Network</ml>.
Il y a plusieurs modules, selon que le programme distant sera le même,
ou bien qu'il sera différent mais compilé avec la même version d'OCaml,
ou encore différent et compilé avec une autre version d'OCaml. Dans
l'exemple ci-dessous, nous utilisons le module <ml>Same</ml> car le programme
distant est le même que celui local.
</p>
<ocaml eval="false">Functory.Network.declare_workers ~n: 4 "machine1";;
Functory.Network.declare_workers ~n: 8 "machine2";;
Functory.Network.Same.map (fun f -> (f, file_size f)) (ls_dir ".") ;;
</ocaml>
<p>
Les programmes sur les machines distantes doivent être lancés et
attendre les instructions du maître avec le code suivant:
</p>
<ocaml>Functory.Network.Same.Worker.compute ();;</ocaml>
<p>
Bien sûr, dans le cas d'utilisation de machines distantes, notre
exemple sur des fichiers se fonctionne que si le maître et
les esclaves partagent le même système de fichiers. De plus, les
calculs à répartir sont en général un peu plus gourmands en temps
et/ou en mémoire...
</p>
<p>
La bibliothèque offre d'autres fonctions que <ml>map</ml> pour effectuer
des calculs (différentes variantes de <em>fold</em>), la possibilité
de paramétrer le port par lequel maître et esclaves communiquent, ...
On consultera la documentation pour en savoir davantage.
</p>
</subsection>

<subsection id="parmap" title="Parmap">
<p>
<ext-a href="http://rdicosmo.github.io/parmap/">Parmap</ext-a>
offre seulement la possibilité d'utiliser plusieurs cœurs de la machine,
mais offre davantage de fonctions, traitant à la fois les listes et
les tableaux.
</p>
<p>Avec nos fonctions <ml>ls_dir</ml> et <ml>file_size</ml> précédentes,
nous pouvons obtenir les tailles des fichiers du répertoire courant
de la façon suivante, en utilisant 6 cœurs:
</p>
<oc-eval>Parmap.parmap ~ncores: 6 (fun f -> (f, file_size f)) (Parmap.L (ls_dir "."));;
</oc-eval>
<p>On consultera avantageusement la
<ext-a href="http://rdicosmo.github.io/parmap/doc/Parmap.html">documentation de Parmap</ext-a>
pour découvrir les fonctions offertes pour paralléliser des calculs.
</p>
</subsection>
</section>

<section id="lwt" title="Lwt: Monades et concurrence">
<p>
La bibliothèque <ext-a href="http://ocsigen.org/lwt/">Lwt</ext-a> (pour
<em>Lightweight threads</em>, ou processus légers légers) permet de simuler
l'utilisation de threads en profitant du fait que certaines opérations
sont bloquantes (les entrées sorties ou l'attente d'autres événements)
et qu'en attendant il est possible d'exécuter d'autres parties du code.
</p>
<subsection id="lwt-principes" title="Principes">
<p>
La bibliothèque Lwt utilise une monade<note>Si on ne connaît pas les monades, on
pourra lire les trois billets d'introduction par Bartosz Milewski:
<ext-a href="http://bartoszmilewski.com/2011/01/09/monads-for-the-curious-programmer-part-1/">1</ext-a>,
<ext-a href="http://bartoszmilewski.com/2011/03/14/monads-for-the-curious-programmer-part-2/">2</ext-a>,
<ext-a href="http://bartoszmilewski.com/2011/03/17/monads-for-the-curious-programmer-part-3/">3</ext-a>.</note> basée sur un type représentant les "threads" Lwt, <ml>Lwt.t</ml>, et les deux opérations
<ml>Lwt.bind</ml> et <ml>Lwt.return</ml>.
</p>
<p>
Au lieu d'utiliser les fonctions d'entrée/sortie habituelles comme <ml>Pervasives.input_line</ml>,
on utilisera les variantes fournies par Lwt, comme <ml>Lwt_io.read_line</ml>. Regardons leurs
type respectifs:
</p>
<oc-eval>Pervasives.input_line;;
Lwt_io.read_line;;
</oc-eval>
<p>
Hormis le fait que <ml>Lwt_io</ml> définit un nouveau type pour les canaux de lecture,
ce qui différencie les deux fonctions est le type de retour. Dans le cas de
<ml>Lwt_io.read_line</ml>, c'est un thread Lwt qui est retourné. Ce thread Lwt sera
en attente de pouvoir lire sur le canal indiqué, et la fonction <ml>Lwt_io.read_channel</ml>
retournera immédiatement sans bloquer.
</p>
<p>
Pour utiliser la ligne lue par la fonction, on utilisera la fonction <ml>Lwt.bind</ml>,
qui prend en paramètre un thread Lwt et une fonction. Cette fonction sera appelée quand
le thread Lwt aura terminé, avec en paramètre la valeur retournée par le thread Lwt:
</p>
<oc-eval>Lwt.bind;;</oc-eval>
<p>La fonction en paramèter de <ml>Lwt.bind</ml> retourne elle aussi un thread Lwt,
ceci afin de pouvoir composer les appels de fonctions.
</p>
<p>
Pour créer un thread Lwt retournant une valeur, il suffit d'utiliser la deuxième opération
de la monade, <ml>Lwt.return</ml>, qui prend une valeur et en fait un thread Lwt qui
est déjà terminé:
</p>
<oc-eval>Lwt.return;;
Lwt.return "hello";;</oc-eval>
<p>
A ce point, on peut se rendre compte qu'il n'y a pas de réel parallélisme, puisque
les threads Lwt ne sont pas créés pour exécuter une fonction en parallèle, comme le
permettent les threads systèmes (avec <ml>Thread.create</ml> évoqué plus haut), mais
pour empaqueter une valeur dans un type (le type de la monade).
</p>
<p>Le "parallélisme" vient du fait que Lwt fournit des fonctions non bloquantes pour les
entrées/sorties, et son moteur s'occupe de créer les threads Lwt représentant les
attentes correspondant à ces entrées/sorties, et de signaler ces threads comme
terminés lorque l'attente est terminée<note>En utilisant des sélecteurs sur les
descripteurs de fichiers concernés. Voir la documentation pour plus de détails.</note>.
Cela signifie que tout ce qui n'est pas en attente d'une entrée/sortie pourra
s'exécuter, ce qui fournit une forme de parallélisme entre ce qui attend et ce qui
n'a pas besoin d'attendre.
</p>
<p>Il n'y a cependant pas de parallélisme entre les parties du code qui n'ont pas besoin
d'attendre. Si une partie met potentiellement du temps à s'exécuter ou si,
pour une raison ou une autre, elle fait appel à une fonction bloquante,
il convient de la faire s'exécuter dans un thread léger séparer. Lwt offre pour
cela la fonction <ml>Lwt_preemptive.detach</ml>.
</p>
<p>Ce modèle évite bien des problèmes de concurrence que l'on rencontre
lorsqu'on est en présence de codes s'exécutant effectivement en parallèle.
</p>
<p>
De plus, Lwt inclut des bibliothèques additionnelles pour faire par exemple
de la programmation événementielle, permettant de créer des threads Lwt représentant
l'attente d'un événement.
</p>
<p>
Enfin, aucun thread en attente ne sera débloqué si l'ordonnanceur (<em>scheduler</em>)
n'est pas lancé, par la fonction <ml>Lwt_main.run</ml>, sur le "thread principal":
</p>
<oc-eval>Lwt_main.run;;</oc-eval>
</subsection>
<subsection id="lwt-syntaxe" title="Syntaxe">
<p>Pour faciliter l'écriture de code utilisant Lwt, la bibliothèque fournit
quelques opérateurs et une extension de syntaxe.
</p>
<p>Plutôt que la fonction <ml>Lwt.bind</ml>, on pourra utiliser deux opérateurs
infixes prenant les mêmes paramètres dans un sens ou dans l'autre:
</p>
<oc-eval><![CDATA[open Lwt;;
(>>=);;
(=<<);;
]]></oc-eval>
<p>Ainsi, les deux codes suivants sont équivalents pour créer un thread Lwt qui,
lorsqu'une ligne aura été saisie sur l'entrée standard, retournera cette ligne
mise en majuscules:
</p>
<oc-eval>Lwt.bind (Lwt_io.read_line Lwt_io.stdin) (fun s -> Lwt.return (String.uppercase s));;
Lwt_io.read_line Lwt_io.stdin >>= fun s -> Lwt.return (String.uppercase s);;
</oc-eval>
<p>La fonction <ml>Lwt.map</ml> permet de ne pas avoir à utiliser <ml>Lwt.return</ml>,
qu'elle utilise pour créer une thread Lwt à partir du résulat d'une fonction:
</p>
<oc-eval>Lwt.map;;</oc-eval>
<p>
Les opérateurs <ml>(>|=)</ml> et <ml>(&lt;|=)</ml> sont équivalents à <ml>Lwt.map</ml>
avec deux façons d'ordonner les paramètres. Ainsi, notre code précédent devient:
</p>
<oc-eval><![CDATA[Lwt.map String.uppercase (Lwt_io.read_line Lwt_io.stdin) ;;
Lwt_io.read_line Lwt_io.stdin >|= String.uppercase;;
String.uppercase =|< Lwt_io.read_line Lwt_io.stdin;;
]]></oc-eval>
<p>
Lwt fournit également une extension de syntaxe Camlp4 facilitant
encore l'écritude de code.
</p>
<!--<p>
Ainsi les deux constructions suivantes sont équivalentes:
</p>
<oc-eval toplevel="false" show-code="false">#use "topfind";;
#require "lwt.simple-top";;
#camlp4o;;
#require "lwt.syntax";;
</oc-eval>
<oc-eval>
let t = Lwt_io.read_line Lwt_io.stdin in
  Lwt.bind t (fun s -> Lwt.return (String.uppercase s));;
lwt s = Lwt_io.read_line Lwt_io.stdin in
  Lwt.return (String.uppercase s);;
</oc-eval>
-->
</subsection>

<subsection id="lwt-threads" title="Manipulation des threads">
<p>
Les threads Lwt sont des valeurs OCaml comme les autres, et peuvent
donc être passées à des fonctions. Lwt offre la possibilité d'effectuer
plusieurs opérations sur ses threads. On peut par exemple demander
l'état d'un thread, qui peut être endormi (en attente, <ml>Sleep</ml>),
terminé avec une valeur de retour (<ml>Return of 'a</ml>) ou terminé en
échec (<ml>Fail of exn</ml>).
</p>
<warning id="lwt-run" title="Utilisation de Lwt_main.run">
<p>
Dans les exemples de code qui suivent, <ml>Lwt_main.run</ml> est utilisé
pour "évaluer" le thread en paramètre, c'est-à-dire faire reprendre les
threads en attente d'un événement (comme une entrée/sortie). En pratique,
dans un programme, on n'utilisera cette fonction que sur le thread "principal"
du programme, qui crée les autres.
</p>
</warning>
<p>
Dans l'exemple suivant, nous créons un pipe dont nous obtenons
le canal de lecture <ml>ic</ml> et le canal d'écriture <ml>oc</ml>.
Ensuite, nous créons un thread Lwt <ml>reader</ml> qui lit un caractère dans le
pipe (et donc attend qu'un caractère soit disponible sur le canal
de lecture). Nous affichons ensuite son état avec <ml>Lwt.State</ml>.
Puis nous écrivons un caractère dans le pipe, et redemandons
l'état du thread <ml>reader</ml>. Nous mettons le tout dans une
fonction que nous passons à <ml>Lwt_main.run</ml>, sinon aucun
thread en attente ne serait débloqué:
</p>
<!--
<oc-eval toplevel="false" show-code="false">#use "topfind";;
#require "lwt.simple-top";;
</oc-eval>
-->
<oc-eval>let state_as_string thr =
  match Lwt.state thr with
  Lwt.Return c -> Printf.sprintf "Return with '%c'" c
| Lwt.Fail _ -> "Fail"
| Lwt.Sleep -> "Sleep"
;;
let run () =
  let ic, oc = Lwt_io.pipe () in
  let reader = Lwt_io.read_char ic in
  Lwt_io.write_line Lwt_io.stdout (state_as_string reader)
  (* afficher l'etat et attendre la fin de l'affichage avec >>= *)
  >>= fun _ ->
    (* ecrire dans le canal et ne pas attendre *)
    ignore(Lwt_io.write_char oc 'a');
    (* bloquer en attendant la terminaison de la lecture dans le canal *)
    reader >>= fun _ ->
      (* afficher le nouvel etat du lecteur *)
      Lwt_io.write_line Lwt_io.stdout (state_as_string reader)
;;
Lwt_main.run (run ());;
</oc-eval>
<!--
<p>Notre <ml>reader</ml> est donc en attente. Ecrivons un caractère
dans le pipe et interrogeons à nouveau l'état:
</p>
<oc-eval>
let ic, oc = Lwt_io.pipe () ;;
let reader = Lwt_io.read_char ic ;;
print_endline (state_as_string reader) ;;
Lwt_main.run (Lwt_io.write_char oc 'a');;
print_endline (state_as_string reader) ;;
</oc-eval>
-->
<p>
En pratique, on n'utilisera que rarement la possibilité de connaître l'état
d'un thread Lwt. Par contre, on utilisera des fonctions agissant en fonction
de l'état de threads Lwt, comme la fonction <ml>Lwt.join</ml> permet d'attendre
la fin de plusieurs threads Lwt ou <ml>Lwt.choose</ml> qui retourne le même
résultat que le thread Lwt qui termine en premier parmi une liste:
</p>
<oc-eval>let t = Lwt.join
  [ Lwt_io.write_line Lwt_io.stdout "message 1" ;
    Lwt_io.write_line Lwt_io.stdout "message 2" ;
  ] >>= fun _ -> Lwt_io.flush Lwt_io.stdout
  in
  Lwt_main.run t ;;
Lwt_main.run (Lwt.choose  [ Lwt.return 0 ; Lwt.return 42 ; Lwt.return 56 ]) ;;
</oc-eval>
<p>
</p>

<exercice id="lwt-commands" title="Exécution de commandes dépendantes">
<p>
Lwt permet de paralléliser les attentes non seulement sur les entrées
sorties mais également sur d'autres processus, comme l'illustre cet exercice.
</p>
<p>
Nous souhaitons exécuter des commandes shell, dont certaines sont
dépendantes d'autres commandes et ne peuvent donc être exécutées
qu'après ces dernières.
</p>
<p>
Plutôt qu'exécuter une à une les commandes après les avoir triées
selon leurs dépendances, nous allons utiliser Lwt pour exécuter
ces commandes le plus possible en parallèle.
</p>
<p>
Ecrire un module <ml>Lwt_commands</ml> offrant l'interface suivante:
</p>
<ocaml defer_="1"><include file="&lt;stog-dir/&gt;/codes/lwt_commands.mli" raw="true"/></ocaml>
<p>
On utilisera notamment des fonctions des modules
<ext-a href="http://ocsigen.org/lwt/api/Lwt_process"><ml>Lwt_process</ml></ext-a>
et <ext-a href="http://ocsigen.org/lwt/api/Lwt_list"><ml>Lwt_list</ml></ext-a>,
ainsi que <ext-a href="http://ocsigen.org/lwt/api/Lwt#VALfail"><ml>Lwt.fail</ml></ext-a>
 pour signaler une erreur.
</p>
<p>
Ce module pourra être compilé par les commandes suivantes:
</p>
<sh>$ ocamlfind ocamlopt -c -package lwt.unix lwt_commands.mli
$ ocamlfind ocamlopt -c -package lwt.unix lwt_commands.ml
</sh>
<p>
On pourra tester le module avec le code suivant. Ce dernier
crée plusieurs commandes. <ml>clean</ml> permet de s'assurer
que les fichiers <code>file1.txt</code>, <code>file2.txt</code> et <code>file3.txt</code>
sont supprimés. La commande principale lance un <ml>ls -l</ml> sur ces
fichiers et échouera donc s'ils n'existent pas. Elle dépend donc
de trois autres commandes, chacune dormant une seconde avant de
créer un fichier vide. Ces trois commandes dépendent également chacun
de la commande <ml>clean</ml>:
</p>
<ocaml defer_="1"><include raw="true" file="&lt;stog-dir/&gt;/codes/lwt_commands_test.ml"/></ocaml>
<p>
En compilant le tout dans un programme <code>lwt-commands</code> avec la
commande suivante:
</p>
<sh>$ ocamlfind ocamlopt -o lwt-commands -package lwt.unix -linkpkg \
  lwt_commands.cmx lwt_command_test.ml</sh>
<p>
nous pouvons vérifier que l'exécution de notre programme parallélise bien
les trois commandes qui créent les trois fichiers, puisque le temps d'exécution
total est d'à peine plus d'une seconde, et non trois si les commandes de création
avaient été exécutées séquentiellement:
</p>
<sh>$ time ./lwt-commands
file1.txt  file2.txt  file3.txt

real  0m1.017s
user  0m0.011s
sys   0m0.015s
</sh>
<solution>
<ocaml defer_="1"><include raw="true" file="&lt;stog-dir/&gt;/codes/lwt_commands.ml"/></ocaml>
</solution>
</exercice>

</subsection>

<subsection id="lwt-conclusion" title="Conclusion">
<p>
Nous n'avons vu qu'un ensemble restreint des possibilités offertes par Lwt. En particulier,
nous n'avons pas évoqué la possibilité d'annuler un thread Lwt, la gestion des exceptions,
les variables locales à un thread Lwt.
</p>
<p>Si l'usage de Lwt présente d'incontestables facilités pour écrire du code fortement
soumis à des attentes longues<note>Lwt est notamment utilisé par
<ext-a href="http://ocsigen.org/">Ocsigen</ext-a>, un serveur web en OCaml qui par nature
passe du temps à attendre et envoyer des messages sur le réseau.</note>, il présente
un inconvénient: les codes qui utilisent une bibliothèque s'appuyant sur Lwt ont tendance
à être "contaminé", les résultats fournis par la bibliothèque en question étant
empaquetés dans les threads Lwt, ce qui a tendance à propager la logique de parallélisation
des attentes.
</p>
<p>Symétriquement, dans un code s'appuyant sur Lwt mais souhaitant interagir avec d'autres
bibliothèques n'utilisant pas Lwt, il conviendra de s'assurer qu'aucune opération longue
de ces bibliothèques ne vient bloquer la parallélisation des attentes offertes par Lwt.
Si c'est le cas, on exécutera les parties potentiellement longues ou bloquantes dans
un thread système séparés grâce à la fonction <ml>Lwt_preemptive.detach</ml>
(et on compilera en utilisant en plus le paquet <code>lwt.preemptive</code>).
</p>
</subsection>
</section>
</contents>
</module>